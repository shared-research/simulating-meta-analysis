---
title: "Understanding meta-analysis through data simulation with application to power analysis"
subtitle: "Supplementary materials"
bibliography: "../files/references.bib"
csl: "../files/apa.csl"
author:
  - Filippo Gambarota
  - Gianmarco Alto√®
output:
  bookdown::pdf_document2:
    toc: true
    latex_engine: xelatex
header-includes:
  - \setcounter{table}{0} 
  - \renewcommand*{\thetable}{S\arabic{table}}
  - \setcounter{figure}{0} 
  - \renewcommand*{\thefigure}{S\arabic{figure}}
  - \setcounter{equation}{0}
  - \renewcommand{\theequation}{S\arabic{equation}}
  - \usepackage{pdflscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \usepackage{xcolor}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      eval = TRUE)
options(width = 60)
```

```{r packages, include = FALSE}
library(dplyr)
library(tidyr)
library(here)
devtools::load_all() # loading all custom functions
```

```{r funs, include=FALSE}
simfuns <- get_funs(here("R", "sim-utils.R"))
```

\newpage

# Introduction

In the main paper, we presented the basic meta-analysis models and simulations. However, in real-world meta-analyses, it is common to find complex data structures such as multilevel and multivariate. As reported in the paper, there could be a different source of dependencies when each study reports multiple effect sizes. Rephrasing the [@Cheung2015-bw, pp. 121--122] taxonomy, the first type of dependency, which we call *multilevel* dependency, arises when multiple independent experiments are associated with the same study. Level 1 (i.e., participants' level) is different, but the effects are likely correlated because the same research group conducted the experiments. The second type of dependency, *multivariate* dependency, arises when multiple effects are collected on the same pool of participants within each study. Now sampling errors within each study are correlated because the same participant is tested multiple times. Finally, we have a dependency on multiple effects at the population level. Two outcomes could be correlated (e.g., the type of
memory tasks) even if the pool of participants is different. The first two types of dependency are the reasons to use a multilevel and multivariate model. The following sections will show two examples to simulate these complex data structures.

# Simulations

```{r sim-setup}
library(dplyr)
library(tidyr)
seed <- 2023 # random seed for simulations
```

In the paper, we used the `mapply()` function, a particularly useful tool in R, to apply the same function (in parallel) with multiple parameters using multiple vectors. In our case, the `sim_study()` function has several parameters (`theta`, `nt`, among others), and we can apply the function multiple times with a combination of parameters. It can be considered essentially a `for` loop but more compact and less verbose.

The following code illustrates the idea of `mapply()` compared to a standard `for` loop. The sim data set contains our pool of $k$ studies, and we want to apply the `sim_study()` function to each dataset row. We can use a `for` loop that is verbose but more explicit or the `mapply()` function that is more compact but less explicit. The computation time and results `for` the two approaches are equivalent, given the appropriate setup.

```{r mapply-vs-for, eval = FALSE}
# using a for loop

# preallocate for speed (not necessary with mapply)
res <- vector(mode = "list", length = nrow(sim))
for(i in 1:length(res)){
  res[[i]] <- sim_study(theta = sim$theta[i], nc = sim$nc[i], nt = sim$nt[i])
}

# using mapply
sim <- mapply(sim_study, sim$theta, sim$nc, sim$nt)

# using sim_studies (with mapply inside)
sim <- sim_studies(sim$theta, sim$nc, sim$nt, data = sim)
```

# Three-level model

Compared to the standard two-level model, the three-level model estimates another source of heterogeneity ($\omega^{2}$) representing the variability within studies. We can easily extend the two-level *random-effects* equation into equation \@ref(eq:three-level-model) from the main paper adding another "adjustment" to the overall effect.

\begin{align}
\begin{gathered}
d_{ij} = \theta_r + \theta_i + \theta_{ij} + \epsilon_{ij} \\
\theta_i \sim N(0, \tau^2) \\
\theta_{ij} \sim N(0, \omega^2) \\
\epsilon_{ij} \sim N(0,\sigma^2_i)
(\#eq:three-level-model)
\end{gathered}
\end{align}

The notation suggests that each effect size belongs to an experiment (effect size) ($j$) nested within a study ($i$). The variability between studies (level 3) is handled by the $\tau^{2}$ parameter and the variability within studies (level 2) is handled by the $\omega^{2}$ parameter. Practically, each study is composed of the average effect size $\theta_{r}$ plus a study-specific adjustment determined by $\tau^{2}$. Then each effect size is composed of the study-specific
effect ($\theta_{r} + \theta_{i}$) plus the adjustment determined by $\omega^{2}$ thus $\theta_{r} + \theta_{i} + \theta_{\text{ij}}$. The number of effect sizes within each study will determine the estimation precision of the $\omega^{2}$ parameter and the number of studies will determine the estimation precision of the $\tau^{2}$ parameter. The
relationship between $\tau^{2}$ and $\omega^{2}$ can be expressed using the intraclass correlation coefficient (ICC) expressed as $ICC = \frac{\tau^{2}}{\tau^{2} + \omega^{2}}$ representing the proportion of total heterogeneity associated with level 2 or level 3. For example, an ICC of 0.5 suggests that 50% of heterogeneity is caused by between-studies variability. On the extreme, an ICC close to 1 suggests that the between-studies variability causes most heterogeneity
(i.e., the effect sizes within each study are very similar).

```{r sim-multilevel}
set.seed(seed)
i <- 50 # Number of studies (level 3)
j <- 5 # Number of effect sizes within each study (level 2)
tau2 <- 0.3 # heterogeneity between studies
omega2 <- 0.1 # heterogeneity within studies
icc <- tau2 / (tau2 + omega2) # real ICC
n <- 30 # sample size for each group, within each study
theta_r <- 0.3 # real average effect size

theta_i <- rnorm(i, 0, sqrt(tau2))
theta_ij <- rnorm(i*j, 0, sqrt(omega2))

sim <- tidyr::expand_grid(
  study = 1:i,
  effect = 1:j,
  nt = n,
  nc = n,
  theta_r = theta_r
)

sim$theta_i <- theta_i[sim$study]
sim$theta_ij <- theta_ij

head(sim)
```

Each study $i$ is repeated $j$ times^[To create a more realistic simulation, the number of effect sizes for each paper could be heterogeneous (e.g., sampled from a Poisson distribution)] along the $\theta_{i}$ while each effect has a unique $\theta_{\text{ij}}$. We can use the same approach as the main paper using the `sim_studies()` function.

```{r}
set.seed(seed)
sim$theta_r_i_ij <- sim$theta_r + sim$theta_i + sim$theta_ij
sim <- sim_studies(theta = sim$theta_r_i_ij, nt = sim$nt, nc = sim$nc, data = sim)
```

Finally we fit a *three-level models* using the `rma.mv` from the `metafor` package. The syntax differs slightly from the rma function and is clearly explained here https://www.metafor-project.org/doku.php/analyses:konstantopoulos2011. The essential part is adding the nested random effect using the `random = ` argument specifying that the `study` variable is nested within the `study` variable with `~ 1|study/experiment`.

```{r}
res <- rma.mv(yi, vi, random = ~1|study/effect, data = sim)
summary(res)
```

As explained above, the model estimates the average effect $\theta_{r}$ and the two heterogeneity parameters. To create a more realistic simulation, we can create some variability in the number of experiments within the same paper, similar to what we did for the number of participants in the main paper. We simulated a three-level model, but it is possible to extend to *n* levels for a more complicated nested structure. In this case, we need more heterogeneity parameters (i.e., more nested "adjustments"), but the overall simulation setup is the same.

# Multivariate model

Compared to a standard two-level random-effects model, the *multivariate model* must consider the correlation between effect sizes collected on the same pool of participants. Authors should report these correlations, but often we need to guess a plausible value for the meta-analysis. Compared to the *two-level* and *three-level* models, we now have multiple outcomes ($p$).

Following the example of the main paper, we have three memory tests collected on the same pool of participants. For this reason we have three real effects ($\theta_{r_{1}}$, $\theta_{r_{2}}$ and $\theta_{r_{3}}$), three heterogeneity ($\tau^2_1$, $\tau^2_2$ and $\tau^2_3$) and the population-level correlations between these outcomes ($\rho_{12}$, $\rho_{13}$ and $\rho_{23}$). Moreover, the sampling errors ($\epsilon_i$) are now correlated; thus each study will have three variances for each outcome ($\sigma^2_1$, $\sigma^2_2$, and $\sigma^2_3$), and the correlations between sampling errors ($\rho_{s12}$, $\rho_{s13}$ and $\rho_{s23}$)^[We used the subscript $s$ for sampling errors to distinguish between between-outcomes correlations and correlations between sampling errors]. Equation \@ref(eq:multivariate-model) formalize the multivariate model for a single study $i$ with three outcomes. The random-effects adjustments ($\theta_i$) to the overall effect and sampling errors are now sampled from a multivariate normal distribution, respectively $\theta_i \sim \mathcal{MVN}(0, \mathrm{T}^2)$ and $\theta_i \sim \mathcal{MVN}(0, \mathrm{V}^2)$.

\begin{align}
\begin{gathered}
\begin{bmatrix}
d_{i1} \\
d_{i2} \\
d_{i3}
\end{bmatrix} 
=
\begin{bmatrix}
\theta_{1} \\
\theta_{2} \\
\theta_{3}
\end{bmatrix}
+
\begin{bmatrix}
\theta_{i1} \\
\theta_{i2} \\
\theta_{i3}
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{i1} \\
\epsilon_{i2} \\
\epsilon_{i3}
\end{bmatrix} \\
\begin{bmatrix}
\theta_{i1} \\
\theta_{i2} \\
\theta_{i3}
\end{bmatrix}
\sim \mathcal{MVN}(0, \mathrm{T}^2) \\
\begin{bmatrix}
\epsilon_{i1} \\
\epsilon_{i2} \\
\epsilon_{i3}
\end{bmatrix}
\sim \mathcal{MVN}(0, \mathrm{V}^2) \\
\mathrm{T}^2 = \begin{bmatrix} 
\tau_1^2 & & & \\ 
\rho_{21}\tau_2\tau_1 & \tau_2^2 & & \\ 
\rho_{31}\tau_3\tau_1 & \rho_{32}\tau_3\tau_2 & \tau_3^2
\end{bmatrix} \\
\mathrm{V}^2 = \begin{bmatrix} 
\sigma_1^2 & & & \\ 
\rho_{s21}\sigma_2\sigma_1 & \sigma_2^2 & & \\ 
\rho_{s31}\sigma_3\sigma_1 & \rho_{s32}\sigma_3\sigma_2 & \sigma_3^2
\end{bmatrix} \\
(\#eq:multivariate-model)
\end{gathered}
\end{align}

We can use the `MASS::mvrnorm()` to generate data from a multivariate normal distribution in R. Compared to `rnorm()`, `MASS::mvrnorm()` requires the vector of means (in this case zero) and the variance-covariance matrix.

```{r sim-multivariate}
set.seed(seed)
p <- 3 # number of outcomes
taus2 <- c(0.3, 0.1, 0.2) # vector of taus
rho <- 0.6 # correlation between outcomes
Tcmat <- rho + diag(1 - rho, nrow = p) # correlation matrix
Tcmat

# correlation matrix to variance-covariance matrix
Tvcov <- diag(sqrt(taus2)) %*% Tcmat %*% diag(sqrt(taus2))
round(Tvcov, 3)

multi_sim <- MASS::mvrnorm(n = 100, mu = rep(0, p), Sigma = Tvcov)
head(multi_sim)

# check the result, close to the simulated values
apply(multi_sim, 2, mean) # mean
apply(multi_sim, 2, var) # variance
cor(multi_sim) # correlation
```

The previous code simulated a *compound symmetry* (CS) structure where all variances ($\tau^2$) and covariances ($\rho\tau_1\tau_2$) are the same. There could be different structures of the $T^2$ matrix that are clearly explained in the `metafor::rma.mv()` documentation (see https://wviechtb.github.io/metafor/reference/rma.mv.html#specifying-random-effects). For the sake of simplicity we will use the CS structure. We can use the `sim_T()` function that given the number of studies $k$, outcomes $p$, the vector of $\tau^2$ and the correlation $\rho$ generates the random-effect adjustments^[The function allow to specify heterogeneous $\tau^2$ values where the CS structure assume the same $\tau^2$ for each outcome. Using a fixed $\rho$ and different $\tau^2$ will create what `rma.mv()` calls a *heteroscedastic compound symmetric structure* thus a matrix where the correlation is the same between outcomes and each outcome has a different $\tau^2$ value].

```{r, echo = FALSE, results='asis'}
print_fun(simfuns$sim_T)
```

```{r}
set.seed(seed)
# compound symmetry
sim_T(k = 10, p = 3, taus2 = c(0.2, 0.2, 0.2), rho = 0.7)

# heteroscedastic compound symmetry
sim_T(k = 10, p = 3, taus2 = c(0.1, 0.3, 0.2), rho = 0.7)
```

For the sampling errors, the matrix $V^{2}$ for the study, $i$ is essentially a $p_{i} \times p_{i}$ matrix, where the diagonal contains the sampling variances, and off-diagonal elements are the covariances. Crucially, sampling errors are correlated within a study but uncorrelated between studies. For this reason, stacking all matrices
from $k$ studies will create a block variance-covariance matrix where the correlations between sampling errors of different studies are fixed to zero. Figure \@ref(fig:fig-block-vcov) depicts the idea of the block-variance covariance matrix for three studies with a different number of outcomes. The block variance-covariance matrix can be easily created using the `metafor::vcalc()` function.

```{r fig-block-vcov, echo=FALSE, fig.cap=get_caption(), eval = TRUE}
knitr::include_graphics("img/block-variance-covariance.pdf")
```

We have all pieces to simulate the multivariate model. We can use the `sim_study_m()` that follows the same idea of the `sim_study()` function but extends to multiple outcomes. We generate a single study with $p$ outcomes where sampling errors are correlated according to a compound symmetry structure^[Also for the $V^2$ matrix we can assume different variance-covariance structures. In this example we are using a CS structure]. In practical terms, we assume that the
correlation at the participant level between multiple outcomes is $r$ and fixed between studies.

```{r fun-sim-study-m, results='asis', echo = FALSE}
print_fun(simfuns$sim_study_m)
```

```{r}
set.seed(seed)
thetas <- c(0.1, 0, 0.7) # three real average outcomes
n <- 30 # number of participant per study
r <- 0.6 # correlation between sampling errors
sim_study_m(thetas, nt = n, nc = n, r = r)
```

As in the main paper, we can create a dataframe with simulation parameters and iterating the `sim_study_m()` producing a meta-analysis dataframe.

```{r}
set.seed(seed)
k <- 100 # number of studies
p <- 3 # number of outcomes per study
rho <- 0.7 # population level correlation between outcomes
r <- 0.5 # correlation between sampling errors
n <- 30 # number of participants per group per study

thetas_r <- c(0.1, 0, 1) # population level real effect sizes
taus2 <- c(0.3, 0.3, 0.3) # population level real tau2s

# creating the dataframe structure
sim <- expand_grid(
  id = 1:k,
  p = 1:p,
  nt = n,
  nc = n
)

head(sim)

# adding the real effects and the random-effect adjustments
sim$theta_r <- rep(thetas_r, k)
sim$theta_i <- sim_T(k, p, taus2, rho)

# average effect + random-effect adjustment
sim$theta_r_i <- sim$theta_r + sim$theta_i

# splitting to list of studies for improving the simulation speed
siml <- split(sim, sim$id)

# simulating the effect sizes for each study
resl <- lapply(siml, function(x) sim_study_m(x$theta_r_i, x$nt, x$nc, r))
res <- dplyr::bind_rows(resl) # combine everything
sim <- cbind(sim, res) # append to the sim dataframe

head(sim)
```

Now, we can use the `metafor::rma.mv()` function to fit the multivariate random-effects model. Compared to the multilevel model, the `metafor::rma.mv()` for a multivariate model uses the block variance-covariance matrix as the `vi` argument and the variable representing each outcome as a moderator estimating the average effect
size. We can create the block variance-covariance matrix. Given that we simulated the raw data, we can calculate and use the observed correlations (within the `sim_study_m()` function) and use a different correlation for each study within the `vcalc()` function (see [https://www.metafor-project.org/doku.php/analyses:berkey1998](https://www.metafor-project.org/doku.php/analyses:berkey1998https://www.metafor-project.org/doku.php/analyses:berkey1998) for an example). Often, these values are not reported; thus, we need to guess a plausible correlation to compute the block variance-covariance matrix. Here we use the $r$ value from our simulation and use the same for each study. We know the underlying model in this case, so our guessed value is correct.


```{r}
# create the block variance-covariance matrix
# cluster is the study (i.e. each "block")
# obs identify each outcome (i.e., each "block" number of rows/columns)
# r is the fixed sampling errors correlation
V <- vcalc(vi = vi, cluster = id, obs = p, rho = r, data = sim)

# first two studies (notice the off diagonal zeros for the correlation between different studies)
round(V, 3)[1:6, 1:6]

# outcome to character with a meaningful prefix
sim$p <- paste0("outcome", sim$p)

# fitting the model
res <- rma.mv(yi, V, mods = ~ 0 + p, random = ~p|id, data = sim, struct = "CS")
summary(res)
```

The output is more complicated compared to previous models. The `Variance Components` is the estimation of the $T^{2}$ matrix. In the two-level random-effects model, we have a single $\tau^{2}$ while in the multivariate model, we have a full variance-covariance matrix. Using the `struct = "CS"` argument, we are forcing the `rma.mv()` function to use a compound symmetry structure, thus estimating a single $\tau^{2}$ and a single $\rho$. Again, this is consistent with our simulation but could be a stringent assumption in a real-world analysis. The crucial part is the `random = ~ p|id` argument that specifies the random-effects structure. The syntax is clearly explained here [https://wviechtb.github.io/metafor/reference/rma.mv.html#specifying-random-effects](https://wviechtb.github.io/metafor/reference/rma.mv.html#specifying-random-effects) The basic idea is to specify the random effect structure as `~ inner|outer` where inner is the outcome variable and outer is the paper variable. The mods = `~ 0 + p` argument specify to estimate the average effect for each outcome (i.e., the $\theta_{r}$ vector of average effects). We are removing the intercept (`~ 0 + p` or equivalently `~ p - 1`) and the model will estimate the mean for each level of the `p` variable.

We could use also another structure for the $T^2$ matrix. We know that our data are generated using a CS structure thus using another structure will be less appropriate. In reality, is not known which is the real data generation model thus we can try different structure. Setting the `struct = "UN"` (unstructured) argument, `rma.mv` try to estimate a variance-covariance matrix where all correlations and variances need to be estimated. Compared to the previous model, we are know estimating 3 heterogeneity and three correlation parameters.


We could also use another structure for the $T^{2}$ matrix. We know that our data are generated using a CS structure; thus, using another structure will be less appropriate. In reality, it is not known which is the real data generation model; thus, we can try different structures. Setting the struct = "UN" (unstructured) argument, rma.mv tries to estimate a variance-covariance matrix where all correlations and variances need to be estimated. We are now estimating three
heterogeneity and three correlation parameters compared to the previous model.

```{r}
# fitting the model
res <- rma.mv(yi, V, mods = ~ 0 + p, random = ~p|id, data = sim, struct = "UN")
summary(res)
```

# Useful resources

- https://www.jepusto.com/simulating-correlated-smds/
- https://www.metafor-project.org/doku.php/metafor
- https://stat.ethz.ch/mailman/listinfo/r-sig-meta-analysis
- https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/

# References

<!-- CAPTIONS -->

(ref:fig-block-vcov) Example of a block variance-covariance matrix for three hypothetical studies. In the example, there are three outcomes ($p = 3$), but not all studies have all outcomes. For this reason, the matrix represents each study as a "block," and covariances between different studies are fixed to zero (because sampling errors are not correlated for different participants). The first study (pink) has three outcomes, thus a $3 \times 3$ matrix where the diagonal contains the sampling variances and off-diagonal elements are the sampling covariances. The second study has only one outcome; thus, the matrix is a single value for the sampling variance. The last study follows the same logic as the first study, with only two outcomes.