---
title             : "Understanding meta-analysis through data simulation with applications to power analysis"
shorttitle        : "Simulating meta-analysis"
author: 
  - name          : "Filippo Gambarota \\orcidlink{0000-0002-6666-1747}"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Via Venezia 8, 35131 Padova (PD)"
    email         : "filippo.gambarota@unipd.it"
    role:         
      - "Conceptualization"
      - "Methodology"
      - "Formal Analysis"
      - "Software"
      - "Writing – Original Draft"
  - name          : "Gianmarco Altoè \\orcidlink{0000-0003-1154-9528}"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Methodology"
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "Department of Developmental and Social Psychology, University of Padova, Italy"
abstract: |
  Meta-analysis is a powerful tool to combine evidence from existing literature. Despite several introductory and advanced materials about organizing, conducting, and reporting a meta-analysis, to our knowledge, there are no introductive materials about simulating the most common meta-analysis models. Data simulation is essential for developing and validating new statistical models and procedures. Furthermore, data simulation is a powerful educational tool for understanding a statistical method. In this tutorial, we show how to simulate equal-effects, random-effects, and meta-regression models and illustrate how to estimate statistical power. In the supplementary materials, we extended these simulations also for multilevel and multivariate models. All materials associated with this article can be accessed on Open Science Framework ([https://osf.io/54djn/](https://osf.io/54djn/)).
  
keywords          : "meta-analysis, monte carlo simulations, power analysis"
wordcount         : "7877"
bibliography      : "../files/references.bib"
floatsintext      : yes
figsintext        : yes
linenumbers       : "`r params$linenumbers`"
draft             : no
mask              : no
#toc               : yes
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
numbersections    : true
nocite: | 
  @r-lang
header-includes:
  - \usepackage{setspace}
  - \captionsetup[figure]{font={stretch=1,footnotesize}}
  - \usepackage{float}
  - \usepackage{orcidlink}

#csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa6"
output            :
  papaja::apa6_pdf:
    latex_engine: xelatex
params: 
  linenumbers: "yes"
  draft: "no"
  finalsection: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      out.width = "80%",
                      fig.width = 9,
                      size = "scriptsize",
                      fig.asp = 0.618,
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = 'H',
                      dev = "tikz")

# chunk size
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r packages, include=FALSE}
library(papaja)
library(metafor)
library(dplyr)
library(tidyr)
library(kableExtra)
library(here)
library(latex2exp)
library(ltxplot)

rcit <- sprintf("(version %s.%s; R Core Team, %s)", R.version$major, R.version$minor, R.version$year)
ltxplot::load_theme_ltx()
devtools::load_all()
seed <- 2023 # general seed for all simulations
```

```{r functions, include = FALSE}
# theme for plots
theme_paper <- function(base_size = 15){
  ltxplot::theme_latex(base_size = base_size)
}

# quick table
qtab <- function(data, caption, ...){
  tab <- data |> 
    apa_table(caption = caption,
              placement = "H",
              font_size = "small",
              ...)
  return(tab)
}
```

```{r data, include=FALSE}
# creating figures
source(here("scripts", "r-img.R"), local = knitr::knit_global())
r_imgs <- readRDS(here("objects", "r-imgs.rds"))
simfuns <- get_funs(here("R", "sim-utils.R")) # getting all funs
```

# Introduction

> "If you do not simulate it, you have not understood it." 

A meta-analysis is an essential tool for combining knowledge from multiple studies quantitatively. Meta-analysis is commonly used together with a systematic review of the literature. The meta-analysis has several advantages. Firstly, it allows combining evidence from multiple studies assigning more weight to studies with lower estimation variability. Then using meta-regression, it is possible to include variables (i.e., moderators) to explain the observed heterogeneity [@Borenstein2009-mo, pp.187-203]. More recently, location-scale models have been developed to inlcude predictors also on the residual heterogeneity [@Viechtbauer2022-rz]. Finally, considering the replication crisis, there are statistical methods to determine the presence and extent of the publication bias. Despite the advantages, meta-analysis implementation is not always straightforward, especially for complex data structures. Additionally, the fact that there are several introductory and advanced resources to understand meta-analysis [@Borenstein2009-mo; @Schmid2022-xe; @Harrer2021-bz], to our knowledge, there are no introductory resources about how to simulate realistic meta-analytic data. Simulating data has several advantages because it requires understanding the statistical method and the data-generation process. Furthermore, data simulation is the primary tool when it comes to evaluating a new analysis method, estimating the statistical power, or understanding the long-run behavior of our data generation process [@Ingalls2011-xt; @Gelman2020-tg, pp. 69-76; @Gelman2006-pc, pp. 155-176]. A recent paper by DeBruine and Barr [-@DeBruine2021-id], which deeply inspired the current work, proposed a stimulating way to understand linear mixed effects via data simulation. Simulating data is also a powerful educational tool within this framework. For these reasons, this work aims to introduce the basic concepts of meta-analysis and Monte Carlo simulations for fixed and random-effects, and meta-regression models with applications also for statistical power calculation. In the first section, we will introduce basic concepts of the meta-analysis that are useful for setting up the simulation. We will evaluate the effect size, variance calculation, and the fixed vs. random effects model distinction. Then we will describe how to simulate data for these models and simulate a meta-regression model with a categorical and numerical predictor. Finally, we will introduce the power analysis extending the previous examples to estimate the statistical power. We used the R statistical programming language `r rcit`. We assume the reader is familiar with the basic concepts of R, but core functions will be explained. Code and materials are available on the OSF repository ([https://osf.io/54djn/](https://osf.io/54djn/)). Multivariate and multilevel models’ simulation examples and more details about the coding approach are available in the supplementary materials.

## Meta-analysis introduction

The meta-analysis is a statistical procedure to combine multiple studies (i.e., *primary studies*) into a single statistical analysis [@Borenstein2009-mo]. The idea is that combining numerous preliminary studies improve the estimation of a particular phenomenon more efficiently compared to conducting a single study. In statistical terms, the concept of the meta-analysis is to switch the statistical unit from the single participant or observation (i.e., *level 1*) to the study (i.e., *level 2*). Given that some studies will give more information because their estimation variability is smaller (e.g., higher sample size), the meta-analysis combines the studies assigning more weight as a function of the precision (i.e., the inverse of the variance).

As an example that will be used throughout the paper, we consider the efficacy of memory training in improving memory performance during a cognitive task. The typical primary study will collect data from a group of participants receiving the memory training (*experimental group*) and another group receiving a control treatment (*control group*). The focus of the meta-analysis is collecting multiple studies with similar aims and methods and estimating the average effect of memory training. Despite differences in the type of cognitive task or experimental setup, each primary study collects an *experimental group* ($n_{C}$) and *control group* ($n_{C}$) and computes the average performance (*experimental group* $\overline T$ and *control group* $\overline C$) and standard deviations (*experimental group* $s_{T}$ and *control group*
$s_{C}$).

### Effect size and variance

The first step of a meta-analysis is to extract information from included studies. This common measure should give an immediate idea of the direction (i.e., the treatment improves or reduces performance) and the size of the effect. Standardized effect size measures [see @Lakens2013-eq for an overview], such as the Standardized Mean Difference (SMD), which could be estimated by Cohen's $d$ [@Cohen1988-ad] or the Pearson correlation coefficient $\rho$ which could be calculated using the associated sample estimator *r* are commonly used to compare heterogeneous outcome variables^[When using bounded effect size indexes such as the Pearson correlation, Fisher's *z* transformation is commonly used[@Borenstein2009-mo, pp. 41-43]]. If all studies used the same raw measure, such as reaction times, it is possible to directly meta-analyze the studies without standardizing using (UMD, unstandardized mean difference).

As reported in the previous section, beyond the effect size of each study, we need to assign a weight according to the precision. For this reason, we need to calculate the sampling variability of the effect size or the raw measure that will represent the estimation precision. Each raw or standardized effect size measure (e.g., raw mean difference, Cohen's $d$, and Pearson's correlation) has a different formula to calculate the sampling variability. The idea is to choose the appropriate measure considering the study design (e.g., between vs. within-subjects) and available information and find the proper formula to compute the sampling variability. In addition, there are formulas and approaches to convert from one effect size measure to another [@Borenstein2009-mo; @Lipsey2001-vp; @Lakens2013-eq].^[See also the blog post by James Pustejovsky ([link](https://www.jepusto.com/alternative-formulas-for-the-smd/)) for a general method to compute the sampling variability for standardized mean difference measures.] Usually, the effect size sampling variability depends mainly on the sample size that determines the weight assigned during the meta-analytic estimation.

### Equal vs fixed vs random-effects model

The core of a meta-analysis is combining the results of multiple studies giving more weight to studies that provide a more precise effect estimation. We can find essentially three meta-analysis models: the equal-effects, the fixed-effects and the random-effects model [@Laird1990-ro, @Hedges1998-uo, see also @Gronau2021-do and @Berkhout2023-fj for Bayesian a meta-analysis approach trying to combine evidence from multiple models e.g., equal and random-effects together]. The *equal-effects* model assume that each study included in the meta-analysis is a more or less precise estimation of the true underlying effect ($\theta$). In other terms, we are assuming that there is no variability (i.e., heterogeneity) among true effect sizes. On the other side, suppose some study-level characteristics (e.g., participants' age, sex, or socioeconomic status) or the experimental paradigm (e.g., type of memory task or difficulty) could impact the treatment effect. The *random-effects* model assume a distribution of real effects with mean $\mu_{\theta}$ and variance $\tau^{2}$. Finally, the *fixed-effects* estimate the average effect of the included pool of studies ignoring the presence of heterogeneity. While the distinction between the equal and fixed-effects model is theoretical, the estimated model is the exactly the same^[see https://wviechtb.github.io/metafor/reference/misc-models.html for a detailed explanation]. The *random-effects* model assume and estimate heterogeneity among effect sizes leading to different model parameters. From an inferential point of view, the *random-effects* model provide unconditional inference to the population of effect sizes while the *fixed-effects* model estimate the average effect of the selected studies, and not population-level parameters. The *equal-effects* model assume a single true population effect to be estimated. In the presence of heterogeneity, using meta-regression is it possible to include some predictors (e.g., the type of experimental setup) to explain effect sizes variability.

```{r img-fixed-vs-random, echo=FALSE, fig.asp=1, fig.cap=get_caption()}
r_imgs$fixed_vs_random
```

### Multilevel and multivariate models

The most straightforward situation for a meta-analysis is when each included study contributes with a single effect size. However, it is common to have multiple effect sizes belonging to the same study creating a situation of dependency between statistical units. There are numerous sources of dependency for a meta-analysis [@Cheung2014-fg; @Cheung2015-bw, pp. 121-122]. The first type of dependency (*multilevel*) consists of multiple effect sizes from independent participants. Despite being collected with different participants, effect sizes from the same study could be correlated. We obtain a multilevel data structure with independent effect sizes in the same study. There are multiple ways to model this data structure [see @Cheung2014-fg for an overview]. Still, the most common way is to use a *multilevel meta-analysis model,* considering the heterogeneity between and within studies. The standard *random-effects* (but also the *equal-effects model*) model is also called a *two-level* model because it combines data after aggregating the participants-level (*first-level*) data. The *three-level* model is commonly used to manage the multilevel data structure [@Cheung2014-fg; @Cheung2019-po]. Compared to the *two-level* model, the *three-level* model estimates two heterogeneity parameters ($\tau^{2}$ and $\omega^{2}$), one for the heterogeneity between studies and the other for the heterogeneity within studies.

A second type of dependency (*multivariate*) consists of multiple effect sizes collected from the same pool of participants. For example, when using two cognitive tasks on the same pool of participants. This is commonly known as *multivariate meta-analysis* because the effect sizes sampling errors are correlated given that participants are assessed multiple times. A *multivariate meta-analysis model* could be used to
consider the correlation between sampling errors.

A third type of dependency arises when true effect sizes are correlated at the population level [@Cheung2015-bw, pp. 121-122]. For example, two different memory tasks could be correlated because the latent psychological constructs are intrinsically correlated. This correlation is present even when other groups of participants perform the two memory tasks (i.e., no *multivariate* dependency) or are administered by different researchers (i.e., no *multilevel* dependency). This type of dependency could be managed by estimating the correlation between outcomes using a *multivariate random-effects model* [@Cheung2015-bw, pp. 127-130]. Figures \@ref(fig:img-multilevel) and \@ref(fig:img-multivariate) depict the conceptual idea of *multilevel* and *multivariate* models.

```{r img-multilevel, echo=FALSE, out.width="80%", fig.cap = get_caption()}
knitr::include_graphics("img/multilevel.pdf")
```

```{r img-multivariate, echo=FALSE, out.width="80%", fig.cap = get_caption()}
knitr::include_graphics("img/multivariate.pdf ")
```

# Simulation

## Monte Carlo simulations

The Monte Carlo methods are controlled experiments [@Gentle2009-cj]. Given a set of fixed parameters, probability distributions, and the possibility of generating random numbers, it is possible to simulate the behavior of an empirical system. Monte Carlo simulations are used for statistical and mathematical problems that cannot be solved analytically.

A straightforward example regards estimating the sampling variability of the mean difference. When calculating the mean difference between two samples, we estimate the true mean difference at the population level with a certain degree of error (i.e., the standard error of the mean difference). The central limit theorem states that the difference between the means of two random samples ($\overline{X}$ and $\overline{Y}$) is approximately normally distributed with mean $\mu_{x} - \mu_{y}$ and standard error $\sqrt{\frac{\sigma^2_x}{{n_x}} + \frac{\sigma^2_y}{{n_y}}}$. The same results can be obtained using Monte Carlo simulations using the following procedure:

1. Generating two random samples from two normal distributions with a fixed mean difference
2. Calculating the mean difference
3. Repeating the same process, many times
4. Calculate the standard deviation of the simulated values

Using the method, we are estimating via simulation the standard deviation of the sampling distribution of the mean difference (i.e., the standard error of the mean difference). Increasing the number of simulations will produce more stable results.

```{r monte-carlo-simulation}
set.seed(123) # seed for simulation to ensure the reproducibility of results
es <- 0.5 # real mean difference in the populations
sigma <- 1 # real standard deviations in the population
n <- 30 # sample size for both groups
nsims <- 1e5 # number of simulations = 100000

# simulates the sampling distribution of the mean difference
d_mc <- replicate(nsims, expr = {
  g1 <- rnorm(n = n, mean = es, sd = sigma)
  g2 <- rnorm(n = n, mean = 0, sd = sigma)
  mean(g1) - mean(g2) # calculates the sample mean difference and returns the value
})

# analytical standard error of the mean difference
se_an <- sqrt(sigma^2/n + sigma^2/n)

# monte-carlo standard error of the mean difference
se_mc <- sd(d_mc)
```

The standard error estimated solving analytically is `r round(se_an, 4)` and using the Monte Carlo simulation we arrive at the same result (i.e., `r round(se_mc, 4)`).

## Simulation setup

This tutorial use several R packages for the simulations, meta-analysis fitting and figures/tables. For the data manipulation we used the *tidyverse* [@R-tidyverse] package. For the models fitting we used the *metafor* [@metafor2010] package. For figures and tables the *ggplot2* [@ggplot22016] or the `metafor::forest()` function, *kableExtra* [@R-kableExtra] and *papaja* [@R-papaja] packages. We set the `seed` for reproducibility to prepare the R environment for the simulations.

```{r sim-setup, eval = FALSE}
library(dplyr) # for data manipulation (within tidyverse)
library(tidyr) # for data manipulation (within tidyverse)
library(metafor) # for meta-analysis models fitting
seed <- 2023 # general seed for all simulations
```

Before diving into the specific simulations, in this section, we define the common aspects of all simulations in the following sections. The current paper focuses on the *two-level*, *fixed,* and *random-effects* models. All the examples refer to primary studies that assess the efficacy of a treatment by comparing a control and an experimental group. There are different simulation approaches to meta-analysis concerning how to generate study-level or participant-level data. In simulation studies where the purpose is not evaluating effect size estimators, it is convenient to simulate unstandardized effect size measures (e.g., UMD). Firstly the estimator is unbiased thus not requiring small sample correction [e.g., @Hedges1989-ip; @Hedges1981-za]. Then the effect size and the sampling variance are independent. Similar to the simulation approach by Viechtbauer [@Viechtbauer2005-yh; @Viechtbauer2007-tq] the *experimental* group ($T$) and *control* groups $C$ are sampled from normal distributions respectively $T_i \sim \mathcal{N}(\Delta, 1)$ and $C_i \sim \mathcal{N}(0, 1)$ where $\Delta$ is the unstandardized mean difference (UMD). The UMD and the sampling variance are calculated using Equations \@ref(eq:effsize) and \@ref(eq:effsizevar) where $y$ is the sample estimator of $\Delta$.

\begin{align}
\begin{gathered}
D = \overline{T} - \overline{C}
(\#eq:effsize)
\end{gathered}
\end{align}

\begin{align}
\begin{gathered}
\sigma_{\epsilon D}^{2} = \frac{s^2_T}{n_T} + \frac{s^2_C}{n_C}
(\#eq:effsizevar)
\end{gathered}
\end{align}

We can use the following algorithm implemented in the `sim_study()` function to simulate a single study. Before using the `sim_study()`
function, we can create a data frame for the simulation using the `make_data()` function^[This function is a simple wrapper of `data.frame()` that given the number of studies and other variables create the data structure for the simulation.]. Table \@ref(tab:sim-data-example-tab) depicts an example of the `make_data()` output.

1. Choose a $\Delta$, $n_{T}$, and $n_{C}$ value.
2. Simulate $n_{T}$ observations from a Gaussian distribution with $\mu = \Delta$ and $\sigma^{2} = 1$ and $n_{T}$ observations from a Gaussian distribution with $\mu = 0$ and $\sigma^{2} = 1$. In this way, the expected difference between groups will be $\Delta$ and the expected variance for each group is 1.
3. Calculate the observed effect size $y$ and the sampling variance $\sigma_{\epsilon}^{2}$.

The simulation approach can be easily extended calculating a standardized effect size measure [e.g., Cohens' $d$ @Cohen1988-ad] and the corresponding sampling variance. For example, after generating data for the two groups the mean difference can be standardized using the pooled standard deviation and applying the appropriate correction [e.g., @Hedges1989-ip; @Hedges1981-za].

```{r fun-sim-study, results='asis', echo=FALSE}
print_fun(simfuns$make_data)
```

```{r fun-make-data, results='asis', echo=FALSE}
print_fun(c(simfuns$sim_study, simfuns$sim_studies))
```

The `es` is the true effect size ($\mu_{\theta}$ for the random-effects model and $\theta$ for the equal-effects model), `nc` and `nt` are the sample size for the control and experimental group, `aggregate` controls if returning the effect size and the corresponding variance or the participant-level data. We can generate a single study with the desired parameters with this function. A suggestion for each simulation step is to generate a large $n$ to reduce the sampling error and check the recovery of simulated parameters. This is a general strategy that can be applied to every simulation. The `sim_studies()` function will iterate through variables in the `...` argument creating the meta-analysis data frame. The `mapply()` function is clearly explained in the supplementary materials.

The following code simulates a single study with $n = 10000$ and check the estimated mean and standard deviation.

```{r sim-single-study, eval = FALSE}
set.seed(seed)
sim_study(es = 0.3, nc = 10000, nt = 10000, aggregate = FALSE)
```

```{r sim-single-study-tab, echo = FALSE}
set.seed(seed)
sim <- sim_study(es = 0.3, nc = 10000, nt = 10000, aggregate = FALSE) |> 
  group_by(group) |> 
  summarise(mean = mean(y),
            sd = sd(y))
```

The control group has a mean of `r sprintf("%.3f (SD = %.3f)", sim$mean[1], sim$sd[1])` and the experimental group has a mean of `r sprintf("%.3f (SD = %.3f)", sim$mean[2], sim$sd[2])` which are remarkably close to the simulated values.

Using the `sim_study()` function multiple times (with the appropriate adjustments) we can generate a series of studies simulating a data set for a meta-analysis (using the `sim_studies()` function). After each example, we will compute the appropriate model (e.g., *fixed* or *random-effects*) using the `metafor` package [@Viechtbauer2010-xz] to check the recovery of simulated parameters^[By default `metafor` use the Wald *z* test on model parameters but other inference methods are available e.g. the Knapp and Hartung [-@Knapp2003-qx] method by setting `test="knha"` within the `rma()` function. This method is useful especially when the number of studies is low (see https://wviechtb.github.io/metafor/reference/misc-recs.html for more details)]. Table \@ref(tab:notation-tab) summarizes the simulation parameters notation in equations and code.

```{r sim-data-example}
k <- 30 # number of studies
n <- 20 # number of participants per group, per study
es <- 0.3 # real effect size
sim <- make_data(k = k, nt = n, nc = n, es = es)
```

```{r sim-data-example-tab, echo = FALSE}
sim |> 
  trim_df(2) |> 
  qtab(caption = get_caption())
```

```{r notation-tab, echo=FALSE, result = "asis"}
notation_table <- tibble::tribble(
     ~`Equation`,     ~`Code`,                                               ~`Description`,
       "$\\theta$",   "theta",                  "Equal-effects model true effect size",
       "$\\mu$",      "mu",                        "Random-effects model true effect size",
        "$\\tau^2$",    "tau2",                            "Effect size heterogeneity",
       "$\\tau^2_r$",   "tau2r",                   "Residual effect size heterogeneity",
  "$\\overline{T}, \\overline{C}$",        "",               "Mean of the experimental/control group",
      "$s_{T, C}$",        "", "Standard deviation of the experimental/control group",
        "$n_{T, C}$",  "nt, nc",        "Sample size of the experimental/control group",
             "$d_i$",      "yi",                                 "Observed effect size",
      "$\\sigma_{\\epsilon_i}^{2}$",      "vi",                          "Observed sampling variance",
       "$\\delta_i$", "deltai",              "Random effect for the study i",
        "$\\beta_0$",      "b0",                            "Meta-regression intercept",
         "$\\beta_1$",      "b1",                                "Meta-regression slope",
               "$k$",       "k",                                    "Number of studies",
     "$\\epsilon_i$",        "",             "Sampling error for the study i"
  )

notation_table$`Code` <- sprintf("\\texttt{%s}", notation_table$`Code`)
notation_table[is.na(notation_table)] <- ""

notation_table |> 
  apa_table(align = c("l", "l", "l"),
            caption = "Variables in the data-generating model and associated R code.",
            escape = FALSE,
            placement = "H",
            note = "The \\texttt{yi} and \\texttt{vi} notation for the observed effect size ($d_i$) and sampling variance ($\\sigma^2_i$) has been used to be consistent with the \\texttt{metafor} notation.")
```

## Equal-effects model

The most basic model to simulate is the *equal-effects model* (FE). As reported in the previous sections, the *equal-effects model* assumes the presence of a single true effect ($\theta$) and the observed variability is because each effect size is an imprecise estimation of the true effect. In other words, the only source of variability is the sampling variability that depends on the variance of the primary studies (i.e., the sample size). Equations \@ref(eq:equal-effects-model1) and \@ref(eq:equal-effects-model2) depicts the *equal-effects* model.

\begin{align}
\begin{gathered}
y_i = \theta + \epsilon_i
(\#eq:equal-effects-model1)
\end{gathered}
\end{align}

\begin{align}
\begin{gathered}
\epsilon_i \sim \mathcal{N}(0,\sigma_{\epsilon_i}^{2})
(\#eq:equal-effects-model2)
\end{gathered}
\end{align}

Each observed effect size ($y_{i}$) is composed of the real effect size $\theta$ plus an error term ($\epsilon_{i}$) that is sampled from a normal distribution with $\mu = 0$ and $\sigma^{2} = \sigma_{\epsilon_i}^{2}$ (i.e., the known sampling variance of the study $i$). As demonstrated in Equation \@ref(eq:effsize), the increase in sample size will decrease the sampling variability. An exact study will essentially have $\theta$ as the observed effect size. Since we are sampling participants' level data, the error component is already included in the `sim_study()` function. To simulate this model in R we can just call the `sim_study()` multiple times according to the number of desired studies ($k$). In addition, we simulate that each primary study will have a sample size of $n = 20$ for both groups. We will discuss later the appropriateness of this assumption.

```{r sim-equal-effects}
set.seed(seed)
k <- 30 # number of studies
n <- 20 # number of participants per group, per study
theta <- 0.3 # real effect size

sim <- make_data(k = k, nt = n, nc = n, es = theta)
sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim) # simulated data
res <- rma(yi = yi, vi = vi, method = "FE", data = sim)
```

We are using the $\theta$ parameter to generate random data using the `sim_study()` function that will introduce the random error component $\epsilon_{i}$. Then we can fit the *equal-effects* meta-analysis model with the `rma` function and `method = "EE"` of the `metafor` package. The parameter we are estimating is $\theta$ which is close to our simulation value (see Table \@ref(tab:res-equal-effects)). Increasing the number of studies ($k$) and/or the number of participants in each study ($n$) will improve estimation reducing the standard error.

```{r res-equal-effects, echo = FALSE}
res_tidy <- rma_tidy(res)
rma_table(res_tidy$res, res_tidy$notes, caption = get_caption())
```

Increasing the sample size for each study will increase the estimation precision thus the variability among studies will be reduced. This can be easily demonstrated by simulating studies with a high sample size as reported in Figure \@ref(fig:equal-effects-high-low-precision). As the sample size increases the only source of variability (i.e., the error component $\epsilon_{i}$) is close to zero so each study is closer to the true simulated value.

```{r equal-effects-high-low-precision, echo = FALSE, fig.asp=0.8, fig.cap=get_caption()}
set.seed(seed)
k <- 10 # number of studies
theta <- 0.3 # real effect size
n <- c(30, 500)

sim <- make_data(k = k, es = theta)

sim <- expand_grid(sim, nc = n)
sim$nt <- sim$nc

sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim)

sim <- escalc(yi = yi, vi = vi, data = sim, measure = "MD")
sim <- summary(sim)

fit_lp <- rma(yi, vi, method = "EE", data = filter(sim, nt == 30 & nc == 30))
fit_hp <- rma(yi, vi, method = "EE", data = filter(sim, nt == 500 & nc == 500))

xlim <- c(min(sim$ci.lb) -0.5, max(sim$ci.ub) + 0.5)

par(mfrow = c(1,2))
forest(fit_lp, main = "N = 30", xlim = c(-2, 3), alim = c(-1, 2))
forest(fit_hp, main = "N = 500", xlim = c(-2, 3), alim = c(-1, 2))
```

## Random-effects model {#re-model}

The *random-effects model* (RE) can be considered an extension of the *equal-effects model*. The *equal-effects model* assumes that the real effect is a single value. The random-effects model relaxes this assumption allowing the true effect size to vary across studies. For example, the difference between groups we are simulating could be influenced by the type of experiment or the participants' age.
Now $\theta$ is no longer a single value but a distribution of values. Due to the effect size being a distribution, we need to estimate both the mean and the variance. The parameter $\mu_{\theta}$ is mean of the distribution, interpreted as the average effect size across different true effect sizes and $\tau^{2}$ is the variance of the distribution interpreted as variability or heterogeneity of effect sizes. In practical terms, we now have two sources of variability: $\tau^{2}$, which express the real difference among effect sizes, and $\sigma_{i}^{2}$ which is the known sampling variance of each study as in the *equal-effects model*. We can easily extend Equation \@ref(eq:equal-effects-model1) with Equations \@ref(eq:random-effect-model1), \@ref(eq:random-effect-model2) and \@ref(eq:random-effect-model3).

\begin{align}
\begin{aligned}
y_i = \mu_{\theta} + \delta_i + \epsilon_i \\
(\#eq:random-effect-model1)
\end{aligned}
\end{align}

\begin{align}
\begin{aligned}
\delta_i \sim \mathcal{N}(0,\tau^2)
(\#eq:random-effect-model2)
\end{aligned}
\end{align}

\begin{align}
\begin{aligned}
\epsilon_i \sim \mathcal{N}(0,\sigma_{\epsilon_i}^{2})
(\#eq:random-effect-model3)
\end{aligned}
\end{align}

Compared to the *equal-effects model* we need to generate another adjustment to the overall effect $\mu_{\theta}$ from a normal distribution with mean 0 and variance $\tau^{2}$. The real effect size for the study $i$ will be $\mu_{\theta} + \theta_{i}$ where $\theta_{i}$ are the random-effects regulated by the $\tau^{2}$ parameter.

```{r sim-random-effect}
set.seed(seed)
k <- 30 # number of studies
n <- 20 # number of participants per group, per study
mu <- 0.3 # real effect size
tau2 <- 0.2 # the effect size heterogeneity

sim <- make_data(k = k, nc = n, nt = n, mu = mu)

# simulate the random-effects adjustment
sim$deltai <- rnorm(k, 0, sqrt(tau2))

# adding the by-study adjustment to the average effect
sim$es <- sim$mu + sim$deltai 

# now we are using mu_deltai and no longer theta
sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim)
res <- rma(yi = yi, vi = vi, method = "REML", data = sim)
```

Then we can fit the *random-effects* meta-analysis model with the `rma` function and `method = "REML"`. Table \@ref(tab:res-random-effect) depicts the model results^[The `method = "REML"` is not the only method to estimate a random-effects model. See the `rma` documentation (https://wviechtb.github.io/metafor/reference/rma.uni.html#specifying-the-model) for an overview of other estimation methods] of the metafor package. We are now estimating two parameters: $\mu_{\theta}$ and $\tau^{2}$. As for the *equal-effects model*, increasing the number of studies and/or the number of participants in each study will improve estimation reducing the standard error.

```{r res-random-effect, echo = FALSE}
res_tidy <- rma_tidy(res)
rma_table(res_tidy$res, res_tidy$notes, caption = get_caption())
```

An important aspect of the *random-effects model* is the interplay between the heterogeneity ($\tau^{2}$) and the sampling variability ($\sigma_{i}^{2}$). As the number of studies $k$ increases, the estimation of $\mu_{\theta}$ and $\tau^{2}$ becomes more precise. Additionally, as each study\'s sample size decreases, each $\theta_{i}$ will be estimated with higher precision but as long as $\tau^{2} \neq 0$ there will be variability among effect sizes. In other words, increasing the sample size of each study or the number of studies will not affect the value of $\tau^{2}$ but only the estimation precision [see @Borenstein2009-mo, Chapter 16, Figure 16.6 for a clear explanation]. This can be easily demonstrated using the previous simulation, increasing each study\'s sample size. Figure \@ref(fig:random-effect-high-low-precision) depicts the same meta-analysis but with different precision in estimating the study $y_{i}$. Compared to Figure \@ref(fig:equal-effects-high-low-precision), increasing the sample size of primary studies improves the estimation of each study without reducing the between-studies heterogeneity.

```{r random-effect-high-low-precision, echo = FALSE, fig.asp=0.8, fig.cap=get_caption()}
set.seed(seed)
k <- 10 # number of studies
mu <- 0.3 # real effect size
tau2 <- 0.2
n <- c(30, 500)

sim <- make_data(k = k, mu = mu)

sim <- expand_grid(sim, nc = n)
sim$nt <- sim$nc

deltai <- rnorm(k, 0, sqrt(tau2))

sim$deltai <- rep(deltai, each = 2)

sim$es <- sim$mu + sim$deltai
sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim)

fit_lp <- rma(yi, vi, method = "REML", data = filter(sim, nc == 30 & nt == 30))
fit_hp <- rma(yi, vi, method = "REML", data = filter(sim, nc == 500 & nt == 500))

par(mfrow = c(1, 2))
forest(fit_lp, main = "N = 30", xlim = c(-3, 4), alim = c(-2, 2))
forest(fit_hp, main = "N = 500", xlim = c(-3, 4), alim = c(-2, 2))
```

The relationship between the sampling error and the heterogeneity can be expressed using the $I^{2}$ statistics [e.g., @Higgins2002-fh] that is the percentage of the total variability $\tau^{2} + \tilde{v}$ that is attributable to real heterogeneity between studies ($\tau^{2}$). In Equation \@ref(eq:isquared), $\tilde{v}$ is the "typical" within-study sampling variability [e.g., @Higgins2002-fh]^[see also here https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate#fn__1 for an overview of the $I^2$ statistics also for complex models].

\begin{align}
\begin{gathered}
I^2 = \frac{\hat{\tau}^2}{\hat{\tau}^2 + \tilde{v}}
(\#eq:isquared)
\end{gathered}
\end{align}

From Equation \@ref(eq:isquared) and Figure \@ref(fig:random-effect-high-low-precision) is clear that if each included study has a considerable sample size the total variability will be reduced, mainly driven by real heterogeneity ($\tau^2$). This is the crucial difference between the equal-effects and the random-effects model [see also @Borenstein2009-mo, pp. 117–122].

Given the interpretation of $I^{2}$ it is possible to simulate a meta-analysis fixing a certain $I^{2}$ value. The only caveat is fixing the $\tilde{v}$. When the sample size of each study is the same, $\tilde{v} = \sigma^2_i$. In the other case, $\tilde{v}$ needs to be calculated from sampling variances reported in Higgins and Thompson [-@Higgins2002-fh] and cannot be easily fixed a priori. However, with the assumption of homogeneous sample size across studies^[The homogeneous sample size of primary studies assumption is commonly used to calculate the statistical power [see @Borenstein2009-mo, Chapter 29]], we can solve Equation \@ref(eq:isquared) for $\tau^{2}$ obtaining the heterogeneity value associated with a certain $I^{2}$ as reported in Equation \@ref(eq:tau2-from-isquared). Table \@ref(tab:res-random-effect-i2) depicts the results of the random-effects model fixing the $I^{2}$ value. Choosing a meaningful $\tau^2$ for the simulation can be sometimes difficult. Beyond fixing the $I^2$ value, one could use a plausible value from the literature. For example van Erp and colleagues [@Van_Erp2017-cf] estimate an empirical $\tau^2$ distribution across several published meta-analyses.

\begin{align}
\begin{gathered}
\tau^2 = - \frac{I^2\tilde{v}}{I^2 - 1}
(\#eq:tau2-from-isquared)
\end{gathered}
\end{align}

```{r sim-random-effect-i2}
set.seed(seed)
k <- 30 # number of studies
mu <- 0.3 # real effect size
n <- 30 # sample size per group, per study
I2 <- 0.6 # desired I2 value

v <- (n + n)/(n * n) + mu^2/(2 * (n + n - 2)) # typical within study variance
tau2 <- -((I2*v)/(I2 - 1))

sim <- make_data(k = k, nc = n, nt = n, mu = mu)

sim$deltai <- rnorm(k, 0, sqrt(tau2))
sim$es <- sim$mu + sim$deltai 
sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim)
res <- rma(yi, vi, method = "REML", data = sim)
```

```{r res-random-effect-i2, echo = FALSE}
res_tidy <- rma_tidy(res)
rma_table(res_tidy$res, res_tidy$notes, caption = get_caption())
```

## Meta-regression {#metareg}

From a linear regression perspective, both the FE and the RE models can be seen as *intercept-only* models where only the mean (i.e., the linear regression intercept) is estimated Borenstein et al. (2009). As reported in the meta-analysis introduction, the between-study heterogeneity usually represents the true variability of the effect due to differences among primary studies. A natural extension of the *intercept-only* analysis is a model that includes variables (i.e., *moderators*) that could explain the observed heterogeneity among effect sizes. For example, a group of studies could use a particular memory task where the expected effect is higher than another. In this way, considering the
type of task will explain part of the observed heterogeneity as in standard regression models. Figures \@ref(fig:img-metaregression-bin) and \@ref(fig:img-metaregression-num) depict a random-effects meta-regression model for a categorical and numerical predictor.

```{r img-metaregression-bin, echo = FALSE, fig.cap=get_caption(), fig.asp=1}
r_imgs$plot_metareg_bin
```

```{r img-metaregression-num, echo = FALSE, fig.cap=get_caption(), fig.asp=1}
r_imgs$plot_metareg_cont
```

### Meta-regression with a categorical moderator

A common example of meta-regression is by including a categorical predictor, including information about study-level features. In our example, a group of studies uses an online memory task while others use a standard lab-based task. The Equation \@ref(eq:random-effect-model1) can be easily extended for a meta-regression model by including a variable encoding the type of task (online vs. lab-based) and the expected difference between the two levels of the moderator (i.e., the *lab vs. online effect*). In regression terms (see equation \@ref(eq:meta-regression)), we could use a dummy variable ($X_{1}$) that takes the value of 0 for the *lab-based task* (*L*) and a value of 1 for an *online task* (*O*)^[The *dummy coding* (also known as treatment coding) is the default in R and `metafor` but other coding schemes could be used [see @Schad2020-ht for an overview of contrast coding schemes]]. Now we fix $\beta_{1}$ to be the *lab vs. online effect* (i.e., the expected mean difference between the two groups of studies) and the product between $\beta_{1}X_{1}$ will consider the *lab vs. online effect*. Crucially, despite $\tau^{2}$ is still the heterogeneity between effect sizes, now we need to fix $\tau^{2}$ considering that we included a moderator. In other terms, $\delta_i \sim \mathcal{N}(0, \tau_{r}^{2})$ where $\tau^2_r$ is the residual heterogeneity after including the moderator. We can describe our model using equation \@ref(eq:meta-regression-equations) according to the value of $X_{1i}$ (L = lab-based experiments, O = online experiments).

\begin{align}
\begin{gathered}
y_i = \beta_0 + \delta_i + \beta_1 X_{1i} + \epsilon_i
(\#eq:meta-regression)
\end{gathered}
\end{align}

\begin{align}
\begin{gathered}
y_{iL} = \beta_0 + \delta_i + \beta_1 \times 0 + \epsilon_i \\
d_{iO} = \beta_0 + \delta_i + \beta_1 \times 1 + \epsilon_i
(\#eq:meta-regression-equations)
\end{gathered}
\end{align}

We can simulate the same scenario of the random-effects model with $k_O = 15$ (*online tasks*) and $k_L = 15$ (*lab-based tasks*). Then we fix the $\beta_1 = 0.2$ and $\tau_r^2 = 0.1$ ($r$ for residual).

```{r sim-meta-reg-dummy}
set.seed(seed)
k <- 30 # the total number of studies
b0 <- 0.1 # intercept, the effect size of the lab-based studies
b1 <- 0.2 # the difference between the two levels of the moderator
tau2r <- 0.1 # the residual heterogeneity
n <- 30 # the sample size per group, per study

sim <- make_data(k = k, nc = n, nt = n, exp = rep(c("lab", "online"), each = k/2))

sim$deltai <- rnorm(k, 0, sqrt(tau2r)) # the by-study residual adjustment
sim$es <- b0 + sim$deltai + b1*ifelse(sim$exp == "lab", 0, 1)

sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim)
res <- rma(yi, vi, mods = ~exp, method = "REML", data = sim)
```

Now we can fit the meta-regression model with the `rma` function as for the random-effects model with the addition of `mods = ~ exp` that indicates which variable/s to consider as moderators. The results are presented in Table \@ref(tab:res-meta-reg-dummy). Now the model will estimate an *intercept* parameter (i.e., $\beta_0$) that is the value of $y$ when $X_1$ is zero (i.e., for lab-based studies) or in other terms the expected value for lab-based studies. Then the $\beta_1$ parameter represents the estimated difference in $y$ between the values of $X_1$ (i.e., lab-based vs online experiments). As said before, $\tau^2_r$ is now the residual heterogeneity that is interpreted as the variability between effect sizes after controlling for the moderator $X_1$.

Now we can fit the meta-regression model with the `rma` function as for the random-effects model with the addition of mods = `~ exp` that indicates which variable/s to consider as moderators. The results are presented in Table \@ref(tab:res-meta-reg-dummy). Next, the model will estimate an *intercept* parameter (i.e., $\beta_{0}$) that is the value of $y$ when $X_{1}$ is zero (i.e., for lab-based studies) or, in other terms, the expected value for lab-based studies. Then the $\beta_{1}$ parameter represents the estimated difference in $y$ between the values of $X_{1}$ (i.e., lab-based vs. online experiments). As mentioned above, $\tau^{2}_r$ the residual heterogeneity is now interpreted as the variability between effect sizes after controlling for the moderator $X_{1}$.

```{r res-meta-reg-dummy, echo = FALSE}
res_tidy <- rma_tidy(res)
rma_table(res_tidy$res, res_tidy$notes, caption = get_caption())
```

### Meta-regression with a numerical moderator

The same approach can be used for a continuous predictor. For example, we can simulate that the average participant's age within each study could explain part of the observed heterogeneity. Now the $X_1$ is a continuous predictor representing the average age for each study and $\beta_1$ is the effect size increase for a unit increase (i.e., 1 year) of average age. Sometimes guessing a plausible $\beta_1$ value with a continuous predictor is not straightforward. A first strategy could be to use values estimated from the literature. Another approach consists in setting up the model and simulating several expected $d_i$ and calculating the range of simulated values. A third possibility is fixing the proportion of explained heterogeneity calculating the $\beta_1$ value accordingly. As in standard regression analysis, we can use the $R^2$ statistic to describe the amount of heterogeneity explained by the included moderators. Equation \@ref(eq:rsquared) reports how to calculate the $R^2$ for a meta-regression model. The $\tau^2_{r}$ is the residual heterogeneity after considering the moderators and $\tau^2_{f}$ is the heterogeneity estimated without considering the moderators. In the next sections, we will present an example of the simulation-based and the $R^2$ based approaches.

\begin{align}
\begin{aligned}
R^2 = 1 - \frac{\tau^2_{r}}{\tau^2_{f}}
(\#eq:rsquared)
\end{aligned}
\end{align}

In terms of regression parameters now the intercept ($\beta_0$) is no longer the overall effect or the average of one category as in the previous example but the estimated value for a specific $X_1$ thus for a specific age. If $X_1$ is a variable representing the average age for each study, then the $\beta_0$ is the average effect size when the age is zero. Depending on the moderator, the intercept is interpreted in different ways. For example, with the age, the intercept has no empirical meaning given that no studies could have a participant average age of zero. A strategy could be to mean-center the age (i.e., subtracting from each study age the average age across the study). Now the intercept is still the average effect size when the age is zero but now zero is the average age. Importantly, the contrast coding for categorical predictors or centering numerical variables does not affect the overall model estimation but only parameters values and interpretation.

#### Assessing the impact of $\beta_1$

As reported in the previous section, a strategy to guess plausible values for $\beta_{1}$ is by simulating several expected $y_{i}$ given the meta-regression equation and summarizing or plotting the effect size range. The range of simulated $y_{i}$ values are also affected by the simulated age values across studies. However, it is probably more intuitive to guess a plausible range of moderator values compared to the $\beta_{1}$ value. In this specific example, if all studies target a specific population (e.g., adults below 50 years), the expected average age range can be easily simulated. In our case, we simulated $k$ average age values from a uniform distribution $\text{ag}e_{i} \sim U\left( 20,40 \right)$. Then we can plot the distribution of $d_{i}$ values to check the plausibility of simulated values. As shown in Figure \@ref(fig:plot-meta-reg-plausible-implausible), with the same range for the moderator, a $\beta_{1} = 0.1$ gives a plausible effect sizes range while a $\beta_{1} = 0.7$ predicts very extreme values.

```{r sim-meta-reg-numerical}
set.seed(seed)
k <- 1000 # number of studies
b0 <- 0.3 # the intercept i.e., average yi when x is 0
b1 <- c(0.1, 0.7) # the beta1 i.e., the increase in yi for an increase in 1 year
tau2r <- 0.1 # the residual heterogeneity after including x1
n <- 30 # number of participants per study, per group
x1 <- runif(k, 20, 40) # random mean-age for each study
x10 <- x1 - mean(x1) # centering the age

sim <- tidyr::expand_grid(id = 1:k, nc = n, nt = n, b1 = b1)

sim$age <- rep(x1, each = 2)
sim$age0 <- rep(x10, each = 2)

deltai <- rnorm(k, 0, sqrt(tau2r))
sim$deltai <- rep(deltai, each = 2)

sim$es <- b0 + sim$deltai + sim$b1*sim$age0

sim <- sim_studies(sim$es, sim$nc, sim$nt, data = sim)
```

```{r plot-meta-reg-plausible-implausible, echo = FALSE, fig.cap=get_caption()}
siml <- split(sim, sim$b1)

plausible_b <- siml[[1]] |> 
  ggplot(aes(y = yi, x = age)) +
  geom_point(alpha = 0.5) +
  ylab(latex2exp::TeX("$d_i$")) +
  xlab("Age") +
  ggtitle(TeX("$\\beta_1 = 0.1$")) +
  #ggthemes::theme_par(base_size = 15) +
  ltxplot::theme_latex(base_size = 15) +
  theme(aspect.ratio = 1,
        plot.title = element_text(hjust = 0.5))

implausible_b <- siml[[2]] |> 
  ggplot(aes(y = yi, x = age)) +
  geom_point(alpha = 0.5) +
  ylab(latex2exp::TeX("$d_i$")) +
  xlab("Age") +
  ggtitle(TeX("$\\beta_1 = 0.7$")) +
  ggthemes::theme_par(base_size = 15) +
  #theme(axis.title.y = element_blank())
  ltxplot::theme_latex(base_size = 15) +
  coord_fixed(ratio = 1) +
  theme(aspect.ratio = 1,
        plot.title = element_text(hjust = 0.5))

plausible_bm <- ggExtra::ggMarginal(plausible_b, type = "hist")
implausible_bm <- ggExtra::ggMarginal(implausible_b, type = "hist")
gridExtra::grid.arrange(plausible_bm, implausible_bm, ncol = 2)
```

#### Simulating using $R^2$

A more intuitive way to simulate a continuous predictor is fixing the desired $R^{2}$ value and finding the coefficient that produces the desired value. This approach has been implemented by Lopez-Lopez and colleagues [-@Lopez-Lopez2014-it]. We can use Equation \@ref(eq:beta-from-r2) to find the $\beta_{1}$ value that is associated with a certain $R^{2}$.

\begin{align}
\begin{aligned}
\beta^2_1 = \tau^2R^2 \\
\tau^2_{r} = \tau^2 - \beta^2_1
(\#eq:beta-from-r2)
\end{aligned}
\end{align}

Now we can simulate the regression model using $\sqrt{\beta^2_1}$ as coefficient and $\tau_{r}^{2}$ as residual heterogeneity. Results from the fitted model fixing the $R^{2}$ values are presented in Table \@ref(tab:res-meta-reg-numerical-r2) and Figure \@ref(fig:plot-meta-reg-numerical-r2). As Lopez-Lopez and colleagues [-@Lopez-Lopez2014-it] demonstrated, to reliably estimate $R^{2}$ the number of studies needs to be large^[see also https://www.metafor-project.org/doku.php/tips:ci_for_r2 for a discussion about confidence intervals for the $R^2$ statistic]. Lopez-Lopez and colleagues [-@Lopez-Lopez2014-it] generated the moderator ($X_{1}$) values from a standard normal distribution. In the following example, we standardized the moderator (`scale()`) after simulating values on the *age* scale
(e.g., `runif(k, 20, 40)`).

```{r sim-meta-reg-numerical-r2}
set.seed(seed)
k <- 100 # the number of studies
r2 <- 0.2 # the desired r2 value
tau2 <- 0.3 # the overall tau2
b0 <- 0.3 # the intercept i.e., average yi when x1 is 0
b1_2 <- tau2 * r2 # the beta1^2 i.e., the increase in yi for an increase in 1 year
b1 <- sqrt(b1_2) # b1_2 is squared, back to the original scale
tau2r <- tau2 - b1_2 # the residual heterogeneity after including x1
n <- 30 # number of participants per study, per group
x1 <- runif(k, 20, 40) # random mean-age for each study

sim <- make_data(k = k, nt = n, nc = n, age = x1)

sim$deltai <- rnorm(k, 0, sqrt(tau2r))
sim$age0 <- scale(sim$age, center = TRUE, scale = TRUE) # standardize the moderator
sim$es <- b0 + sim$deltai + b1*sim$age0

sim <- sim_studies(sim$es, sim$nt, sim$nc, data = sim)
res <- rma(yi, vi, mods = ~age0, method = "REML", data = sim)
```

```{r res-meta-reg-numerical-r2, echo = FALSE}
res_tidy <- rma_tidy(res)
rma_table(res_tidy$res, res_tidy$notes, caption = get_caption())
```

```{r plot-meta-reg-numerical-r2, echo = FALSE, fig.cap=get_caption()}
res <- rma(yi, vi, mods = ~age, data = sim)
sim$wi <- weights(res) # meta-analysis weights

age_p <- seq(20, 40, 0.01) # age values to compute predictions
preds <- predict(res, newmods = age_p)
preds$age <- age_p
preds <- data.frame(preds)

ggplot() +
  geom_line(data = preds,
            aes(x = age, y = pred)) +
  geom_ribbon(data = preds,
              alpha = 0.2,
              aes(x = age, y = pred, 
                  ymin = ci.lb, ymax = ci.ub)) +
  geom_point(data = sim, 
             aes(x = age, y = yi, size = wi),
             show.legend = FALSE) +
  ylab(latex2exp::TeX("$d_i$")) +
  xlab("Age") +
  theme_paper()
```

# Power Analysis

The previous simulation examples can be easily implemented for multiple purposes. For example, we can use different effect sizes and variance estimators when using the `sim_study()` function to check the impact on the fitted meta-analysis model. However, one of the most critical applications is estimating the power of a specific statistical model.

As explained in the introduction, there are several approaches and tools to estimate the power of *fixed* and *random-effects models* [see @Borenstein2009-mo, Chapter 29; @Harrer2019-rl, Chapter 14]. These methods are easy to implement but made strong assumptions, such as the homogeneity of sample size, and did not consider the uncertainty in estimating $\tau^{2}$. Jackson and colleagues [-@Jackson2017-dv]
partially solve the issue by developing an interesting method that takes into account the uncertainty in estimating $\tau^{2}$ without using simulations. However, only Monte Carlo simulations can consider complex scenarios and extra parameters. 

A general Monte Carlo simulation for the power analysis can be implemented with the following steps:

1.  Choose the model that generates the data (e.g., fixed, or random-effects model).
2.  Fix the relevant parameters (e.g., $\tau^{2}$ and $\theta$).
3.  Simulate a dataset.
4.  Fit the appropriate model.
5.  Store the p-value associated with the parameter of interest
6.  Repeat 3-5 a large number of times (e.g., 10000).
7.  Calculate the power as the proportion of p-values below the $\alpha$ level.

For example, we can estimate the power of a *random-effects model* by repeating the simulation presented in Section \@ref(re-model) many times. We simulated heterogeneity of sample sizes sampling $n_{T}$ and $n_{C}$ values from a Poisson distribution with $\lambda = 20$. In this way, on average, the sample size is 20 for primary studies with a certain amount of heterogeneity. Usually, it is more informative to simulate different scenarios according to the relevant parameters, such as sample sizes, number of studies, or heterogeneity. For example, we can estimate the power with a different number of studies $k$. We define the `do_sim()` function that, according to the input parameter, repeats the simulation a certain number of times (i.e., `nsim`)^[We are using the `purrr::pmap()` function that can be considered very similar to `mapply()` but less verbose. Compared to `mapply()`, the `sim_grid` columns are directly passed to the `do_sim()` function without specifying the order.]. Increasing the number of simulations will increase the power analysis estimation precision. Then the `summary_sim()` function analyzes each simulation returning the relevant values. We repeat the simulation of the *random-effects model* several times with different parameters.

```{r fpower-analysis-functions, results='asis', echo=FALSE}
print_fun(c(simfuns$do_sim, simfuns$summary_sim))
```

Simulation results are presented in Figure \@ref(fig:plot-power-analysis) and Table \@ref(tab:tab-power-analysis-example) showing that to reach 80% power (usually considered an appropriate level) with $\alpha = 0.05$ we need ~35 studies. The same approach could be used to estimate the power of a meta-regression by simply modifying the `do_sim()` function simulating the effect of a moderator and extracting the relevant p-value.

```{r power-analysis-example, cache = TRUE}
set.seed(seed)
nsim <- 5000 # number of simulations per condition (5000, higher is better)
k <- c(5, 15, 25, 35, 50) # number of studies
delta <- 0.3 # the average effect size
tau2 <- 0.3 # the heterogeneity
navg <- 20 # average sample size per study
nmin <- 10 # minimum sample size per study
alpha <- 0.05 # the alpha level

# creating all combinations
sim_grid <- tidyr::expand_grid(k, mu, tau2, navg, nmin, nsim)

# apply the simulation to all combinations
res <- purrr::pmap(sim_grid, do_sim)

# combine the results
res <- dplyr::bind_rows(res)
sim_grid <- cbind(sim_grid, res)
```

```{r tab-power-analysis-example, echo = FALSE}
sim_grid$navg <- as.integer(sim_grid$navg)
sim_grid$k <- as.integer(sim_grid$k)
sim_grid$nmin <- as.integer(sim_grid$nmin)
sim_grid$nsim <- as.integer(sim_grid$nsim)

sim_grid_t <- sim_grid
names(sim_grid_t) <- c("k", "$\\Delta$", "$\\tau^2$", "$n_{avg}$", "$n_{min}$", "nsim", "Power")

sim_grid_t |> 
  qtab(caption = get_caption(), escape = FALSE)
```

```{r plot-power-analysis, echo = FALSE, fig.cap=get_caption()}
title <- latex2exp::TeX(sprintf("$n_{avg} = %.1f$, $\\tau^2 = %s$, $\\Delta = %s$, $\\alpha = 0.05$", sim_grid$navg[1], tau2, delta, 0.05))

sim_grid |> 
  ggplot(aes(x = k, y = power)) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "firebrick4") +
  geom_line() +
  geom_point(size = 4, shape = 21, color = "white", fill = "black", stroke = 3) +
  theme_paper() +
  ylim(c(0, 1)) +
  xlab("Number of studies (k)") +
  ylab("Power") +
  ggtitle(title)
```

# Conclusions

The present work introduced the basic concepts of the meta-analysis regarding fixed, random-effects models and meta-regression with a simulation-based approach. We believe the presented examples can help implement alternative or more complex models. For example, the `sim_study()` function can be easily modified to simulate another effect sizes index, such as a paired sample Cohen's $d$ or a Pearson's
correlation. In addition, more complex models, such as *multivariate* or *multilevel* models, can be implemented following the same approach (see supplementary materials). For example, the *three-level* model estimates another heterogeneity component representing the variability of multiple effect sizes in the same study. Similarly, the *multivariate* model could include the correlation between multiple outcomes and the correlation between sampling errors.

The present work did have a few limitations. First, we only introduced basic concepts about meta-analysis and Monte Carlo simulations, while setting up complex simulations requires more knowledge and complexity of the simulation setup. We decided to give the foundations to understand meta-analyses with a simulation approach because more complex models are still based on the same principles. Second, there are limitations
concerning simulating participant-level data. We decided to simulate the meta-analysis dataset starting from the participants' level to maximize the flexibility and clearness of each step. The downside concerns the efficiency and scalability of the simulation setup. For large-scale simulations (e.g., many conditions, iterations, or complex models), simulating from aggregated statistics is probably more efficient (see
Heuvel et al., 2020 for an example) to improve the computation speed^[see https://www.jepusto.com/simulating-correlated-smds/ for a very clear example of the participant-level vs aggregated data simulation].

In conclusion, data simulation is a very powerful tool for each step of a data analysis process. Starting from the learning phase, where simulating data can be used to understand the statistical model in terms of assumptions and the data generation process, to the estimation of statistical power. Moreover, we believe that data simulation as part of a standard research workflow could improve the overall research quality. Data simulation requires understanding the statistical model, setting appropriate and reasoned parameters, and realizing how the chosen analysis method behaves across different scenarios.

\newpage

### Acknowledgments {-}

We thank the [R-sig-meta-analysis](https://stat.ethz.ch/mailman/listinfo/r-sig-meta-analysis) mailing-list where suggestions and clarifications significantly improved our simulations approach and the R code.

### Conflicts of Interest {-}

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

### Data, materials, and online resources {-}

The code to reproduce simulations, figures and tables can be found on Open Science Framework (https://osf.io/54djn/)

### Supplemental Material {-}

### Prior versions {-}

The manuscript preprint has been uploaded on PsyArXiv [https://psyarxiv.com/br6vy/](https://psyarxiv.com/br6vy/)

\newpage

# References {-}

::: {#refs custom-style="Bibliography"}
:::

<!-- FIGURE AND TABLES CAPTIONS -->

(ref:img-equal-vs-random) The difference between the assumptions of the *equal* and *random-effects model*. Each distribution depicts the sampling distribution of $k = 4$ hypothetical studies ($i = 1$, $2$, ..., $4$) with a certain observed effect size $y_{i}$ (red squares), sampling variability $\sigma_{i}^{2}$ and the 95% confidence interval (black segment). The *equal-effects* plot on the left suggests that each observed effect size has the same underlying true effect $\theta$ (each distribution has the same mean) with a different degree of precision (e.g., Study 1 is more precise than Study 3). In practice, each study has a different observed effect size where studies with high precision (i.e., narrow sampling distributions) will be remarkably close to the real effect ($\theta$). The *random-effects model* on the right suggests that beyond the error term ($\epsilon_{i}$), each real effect size is composed of a fixed part (now $\mu_{\theta}$) and a random part ($\theta_{i}$) sampled from a normal distribution with mean $\mu_{\theta}$  and variance $\tau^{2}$. When $\tau^{2}$ is zero, the random-effects reduces to a equal-effects model.

(ref:img-multilevel) Graphical representation of a *multilevel* (three-level) model. On the top, the effect sizes are distributed at the population level. Then, each study is defined as the overall effect and the random effects determined by $\tau^{2}$, $\theta_{r} + \theta_{i}$. Finally, each experiment (effect size) within a study is composed of the study-specific effect $\theta_{r} + \theta_{i}$ and the experiment-specific random-effects $\theta_{r} + \theta_{i} + \theta_{\text{ij}}$ determined by $\omega^{2}$.

(ref:img-multivariate) Graphical representation of a *multivariate* model. In this example, we have two outcomes (magenta and green) with an average effect, heterogeneity parameters, and a certain true correlation value ($\rho$, third type of dependency). Each study at level-2 (as in the standard random-effects model) has two effect sizes with corresponding sampling variances. The two effect sizes are correlated ($\rho_{s}$, with $s$ for sampling errors) because each effect is collected on the same participants.

(ref:sim-data-example-tab) Example of data generated with the `make_data()` function. The `id` column is the identifier for each study.

(ref:notation-tab) Overview of math notation and the corresponding variables names used in the code examples.

(ref:res-equal-effects) Summary of the simulated equal-effects model. The only estimated parameter is the average effect* $\theta_{f}$ *with the standard error,
95% confidence interval, and the Wald z-test. The z-test evaluates the null hypothesis that the real effect equals zero.

(ref:equal-effects-high-low-precision) Forest plots of two simulated *equal-effects models*. On the left, the simulated model has $nt,nc = 30$ for each included study while on the right the sample size for each study is $nt,nc = 500$. Given that the data were generated under a *equal-effects model*, when the sample size is high (on the right) each study is aligned on the real effect size because the error component ($\epsilon_{i}$) is almost entirely reduced. The average effect size is similar (depending on the random numbers generation) between the two scenarios while the estimation precision (the width of the black diamond) is narrower on the right.

(ref:res-random-effect) Summary of the simulated random-effects model: Compared to the equal-effects model, there are more parameters. The $\beta$ is the average effect ($\theta_{r}$) with the standard error, 95% confidence interval, and the Wald z-test. The $\tau^{2}$ is the estimated heterogeneity and the $I^{2}$ (explained in Section \@ref(re-model)) represents the percentage of total variability due to between-study heterogeneity.

(ref:random-effect-high-low-precision) Forest plots of two simulated *random-effects model*s. On the left, the simulated model has $nt,nc = 30$ for each included study, while on the right, the sample size for each study is $nt,nc = 500$. Compared to Figure \@ref(fig:equal-effects-high-low-precision), data were generated under a *random-effects model*. The estimated average effect is similar between the two scenarios regarding average effect and precision. However, compared to the *equal-effects* simulation, increasing the sample size of primary studies only affects the precision without reducing the true heterogeneity (i.e., $\tau^{2} \neq 0$).

(ref:res-random-effect-i2) Summary of the *random-effect* model fixing the $I^2$ value. Parameters are the same as described in Table \@ref(tab:res-random-effect).

(ref:img-metaregression-bin) Graphical representation of a *random-effects meta-regression* model with a categorical predictor (Condition A and Condition B). Each grey distribution represents the sampling distribution of included studies. The dotted line is the average effect (i.e., random-effects model without moderators). The effect size differs between conditions A and B, including the *condition* moderator explaining part of the total heterogeneity (red plus green segments). The green segments depict the explained heterogeneity, and the red segments the residual (unexplained) heterogeneity.

(ref:img-metaregression-num) Graphical representation of a *random-effects meta-regression* model with a numerical predictor ($x$). Each grey distribution represents the sampling distribution of included studies. The dotted line is the average effect (i.e., random-effects model without moderators). The effect size increases as a function of the $x$ variable. Therefore, including $x$ as a predictor explains the
heterogeneity. The rest of the Figure follows the same logic as Figure \@ref(fig:img-metaregression-bin).

(ref:res-meta-reg-dummy) Summary of the *random-effect* model with a categorical predictor (lab vs online experiments). The `intercept` is the average effect for lab-based experiments and `exponline` is the difference between lab-based and online experiments. The $R^2$ is the percentage of explained heterogeneity and $\tau^2_r$ is the estimated residual heterogeneity. Other parameters are the same as the standard *random-effect* model (see Table \@ref(tab:res-random-effect)).

(ref:res-meta-reg-numerical-r2) Summary of the *random-effects* model fixing the $R^{2}$ value. The `intercept` is the effect size for the average `age` (given that age is mean-centered). The `age0` parameter is the slope between `age` and the effect size interpreted as an increase in effect size for a unit increase in the average age. Other parameters are the same as described in Table \@ref(tab:res-random-effect) and \@ref(tab:res-meta-reg-dummy).

(ref:plot-meta-reg-plausible-implausible) Scatter plots with marginal histograms for the range of simulated $d_i$ with two $\beta_1$ values. The x-axis depicts the average age of simulated studies and the y-axis depicts the simulated effect size. On the left, the majority of simulated values range between -1.5 and 1.5 thus $\beta_1 = 0.1$ can be considered a plausible value. On the right, ($\beta = 0.7$) values range between -10 to 10 that despite theoretically possible is highly implausible values in real meta-analysis for psychological data.

(ref:plot-meta-reg-numerical-r2) Meta-regression results for the *random-effects model* with a numerical moderator. Each effect size is represented with a black dot where the dimension represents the weight according to the inverse of the precision. The line represents the estimated meta-regression slope with the 95% confidence interval (grey bands).

(ref:plot-power-analysis) Results from the *random-effects model* power analysis. The x-axis depicts the number of studies ($k$), and the y-axis the estimated power. The red dotted line is the 80% power level, usually considered a good value for power analysis.

(ref:tab-power-analysis-example) The results from the power analysis simulation. The table depicts the simulation parameters, the estimated power using the `summary_sim()` function, and the average sample size ($n$) across the simulations.